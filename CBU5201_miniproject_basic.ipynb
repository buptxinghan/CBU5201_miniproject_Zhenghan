{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaGn4ICrfqXZ"
      },
      "source": [
        "# 1 Author\n",
        "\n",
        "**Student Name**: Zhong Zhenghan \n",
        "\n",
        "**Student ID**:  210982480\n",
        "\n",
        "**Github**:  https://github.com/buptxinghan/CBU5201_miniproject_Zhenghan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o38VQkcdKd6k"
      },
      "source": [
        "# 2 Problem formulation\n",
        "- 微笑检测：确定图片中的人是否在微笑。\n",
        "- 3D 头部姿态估计：预测头部的3D方向。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3BwrtEdLDit"
      },
      "source": [
        "# 3 Machine Learning pipeline\n",
        "\n",
        "- 输入：genki4k 数据集中的一张图片。\n",
        "- 转换：使用 Haar 级联或 CNN 进行人脸检测和裁剪，数据标准化和增强，使用 CNN 提取特征。\n",
        "- 模型：微笑检测的分类模型和3D头部姿态的回归模型。\n",
        "- 输出：微笑/非微笑的二元标签和3D头部姿态标签。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1nDXnzYLLH6"
      },
      "source": [
        "# 4 Transformation stage\n",
        "\n",
        "- 图像大小调整：所有图像都被调整为统一的大小,数据标准化（像素值缩放）和增强（旋转，翻转等）。\n",
        "- 使用预训练模型（如 Haar 级联或 CNN）进行面部检测和裁剪。\n",
        "- 颜色空间转换：如果需要，图像颜色空间从 RGB 转换为灰度。\n",
        "- 特征工程：提取与微笑和头部姿态相关的特征,使用大型数据集预训练的 CNN（例如 VGG16）提取特征。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F5_kI95LuZ2"
      },
      "source": [
        "# 5 Modelling\n",
        "\n",
        "- 微笑检测：二元分类模型，例如逻辑回归、SVM 或简单的 CNN。\n",
        "- 3D 头部姿态估计：回归模型，可以是多层感知器或 CNN。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPTSuaB9L2jU"
      },
      "source": [
        "# 6 Methodology\n",
        "\n",
        "- 数据划分：训练/验证/测试数据集比例为70%/15%/15%。\n",
        "- 交叉验证：使用交叉验证来评估模型的泛化能力。\n",
        "- 性能指标：微笑检测的准确度和混淆矩阵；姿态估计的均方误差。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZQPxztuL9AW"
      },
      "source": [
        "# 7 Dataset\n",
        "\n",
        "- 预处理 genki4k 数据集，可视化一些样本。\n",
        "- 执行任何必要的清理或增强。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qf7GN1aeXJI"
      },
      "source": [
        "# 8 Results\n",
        "\n",
        "- 展示模型的性能：包括训练过程中的损失和准确度变化图、验证集和测试集上的最终结果。\n",
        "- 分析结果：讲述模型表现好的地方，以及可能的原因。同时，如果模型某些方面表现不佳，提出可能的原因和解决办法。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSrJCR_cekPO"
      },
      "source": [
        "# 9 Conclusions\n",
        "\n",
        "- 概述您的项目和实验结果。\n",
        "- 讨论项目的成功点和存在的局限。\n",
        "- 提出改进模型或实验的可能方法。\n",
        "- 反思整个项目过程，包括学到了什么，遇到了哪些挑战，以及如何克服这些挑战。\n",
        "\n",
        "在这个项目中，我们成功地实现了一个用于识别人脸微笑的模型。通过使用卷积神经网络，我们在测试集上达到了 XX% 的准确率。尽管结果是令人满意的，但我们发现模型在处理不同光照条件下的图像时存在一定的局限性。未来的工作可以在数据增强和模型鲁棒性方面进行更多的探索。在这个过程中，我们学习了如何处理图像数据、如何选择合适的模型结构以及如何调整模型参数来优化性能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10 Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 初始化人脸检测器\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def crop_face(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    \n",
        "    for (x,y,w,h) in faces:\n",
        "        roi_color = img[y:y+h, x:x+w]\n",
        "        return roi_color\n",
        "    return None  # 如果没有检测到人脸，返回None\n",
        "\n",
        "# 示例 - 裁剪第一张图片\n",
        "image_path = 'D:/Documents/Junior1/Machine Learning/LAB/Mini Project/genki4k/files/file0001.jpg'\n",
        "cropped_image = crop_face(image_path)\n",
        "\n",
        "# 将裁剪的图片展示出来（如果需要）\n",
        "cv2.imshow('Cropped Face', cropped_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 划分数据集\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Extract the feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Classification or Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 实例化标签编码器\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# 将训练和验证标签转换为数值型\n",
        "y_train_encoded = label_encoder.fit_transform(y_train).astype('float32')\n",
        "y_val_encoded = label_encoder.transform(y_val).astype('float32')\n",
        "#y_train_encoded = to_categorical(label_encoder.fit_transform(y_train))\n",
        "#y_val_encoded = to_categorical(label_encoder.transform(y_val))\n",
        "history = model.fit(X_train, y_train_encoded, batch_size=32, epochs=10, validation_data=(X_val, y_val_encoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存模型\n",
        "model.save('smile_detection_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.  Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 获取模型的预测结果\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)  # 假设 y_test 是 one-hot 编码格式\n",
        "\n",
        "# 生成并打印分类报告\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "# 生成混淆矩阵\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# 可视化混淆矩阵\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Smiling', 'Smiling'], yticklabels=['Not Smiling', 'Smiling'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Analyis the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 假设 history 是 model.fit() 的返回值\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 绘制准确率曲线\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "# 绘制损失曲线\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "yolov8",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
